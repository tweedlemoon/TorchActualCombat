{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "2f7ba2d2",
   "metadata": {},
   "source": [
    "# 4.2 模型参数的访问、初始化和共享"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "1bfd4023",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch import nn\n",
    "from torch.nn import init"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "91422f34",
   "metadata": {},
   "source": [
    "net.named_parameters()是net（nn.Module）中的一个固有变量，其形式是一个dict，所以可以通过下面的语句访问"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "19ea1087",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sequential(\n",
      "  (0): Linear(in_features=4, out_features=3, bias=True)\n",
      "  (1): ReLU()\n",
      "  (2): Linear(in_features=3, out_features=1, bias=True)\n",
      ")\n",
      "<class 'generator'>\n",
      "0.weight torch.Size([3, 4])\n",
      "0.bias torch.Size([3])\n",
      "2.weight torch.Size([1, 3])\n",
      "2.bias torch.Size([1])\n",
      "weight torch.Size([3, 4]) <class 'torch.nn.parameter.Parameter'>\n",
      "bias torch.Size([3]) <class 'torch.nn.parameter.Parameter'>\n"
     ]
    }
   ],
   "source": [
    "net = nn.Sequential(nn.Linear(4, 3), nn.ReLU(), nn.Linear(3, 1))  # pytorch已进行默认初始化\n",
    "\n",
    "print(net)\n",
    "X = torch.rand(2, 4)\n",
    "Y = net(X).sum()\n",
    "\n",
    "# 访问模型的参数\n",
    "print(type(net.named_parameters()))\n",
    "for name, param in net.named_parameters():\n",
    "    print(name, param.size())\n",
    "\n",
    "# 一个意思，访问Sequense中第0位网络中的变量\n",
    "for name, param in net[0].named_parameters():\n",
    "    print(name, param.size(), type(param))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "91c2f2d5",
   "metadata": {},
   "source": [
    "这个例子的意思是，一定是nn.Parameter才会被添加到变量区，一个普通的tensor是没有这个资格的"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "86d4dd98",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "weight1\n",
      "tensor([[ 0.4292, -0.3711,  0.4402,  0.2286],\n",
      "        [-0.0336, -0.4182,  0.4068, -0.3192],\n",
      "        [-0.0911,  0.4797, -0.0509, -0.1128]])\n",
      "None\n",
      "tensor([[-0.3794, -0.2619, -0.5702, -0.3841],\n",
      "        [ 0.0638,  0.0419,  0.0635,  0.0045],\n",
      "        [-0.1511, -0.0993, -0.1505, -0.0106]])\n"
     ]
    }
   ],
   "source": [
    "# nn.Parameter是Tensor的子类，其本质就是tensor\n",
    "class MyModel(nn.Module):\n",
    "    def __init__(self, **kwargs):\n",
    "        super(MyModel, self).__init__(**kwargs)\n",
    "        self.weight1 = nn.Parameter(torch.rand(20, 20))\n",
    "        self.weight2 = torch.rand(20, 20)\n",
    "\n",
    "    def forward(self, x):\n",
    "        pass\n",
    "\n",
    "\n",
    "n = MyModel()\n",
    "for name, param in n.named_parameters():\n",
    "    print(name)\n",
    "\n",
    "# 对Tensor的操作都可以对它做\n",
    "weight_0 = list(net[0].parameters())[0]\n",
    "print(weight_0.data)\n",
    "print(weight_0.grad)  # 反向传播前梯度为None\n",
    "Y.backward()\n",
    "print(weight_0.grad)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "eb40159d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.weight tensor([[-0.0080, -0.0180,  0.0148, -0.0014],\n",
      "        [ 0.0089, -0.0004,  0.0105, -0.0167],\n",
      "        [ 0.0063,  0.0090, -0.0010,  0.0020]])\n",
      "2.weight tensor([[-0.0064,  0.0173, -0.0087]])\n",
      "0.bias tensor([0., 0., 0.])\n",
      "2.bias tensor([0.])\n",
      "0.weight tensor([[-0.0000, -0.0000, -9.3368,  0.0000],\n",
      "        [ 0.0000,  5.6437,  0.0000,  0.0000],\n",
      "        [ 8.7269, -0.0000, -8.5961, -5.6223]])\n",
      "2.weight tensor([[-0.0000, 7.9383, -0.0000]])\n",
      "0.bias tensor([1., 1., 1.])\n",
      "2.bias tensor([1.])\n"
     ]
    }
   ],
   "source": [
    "net = nn.Sequential(nn.Linear(4, 3), nn.ReLU(), nn.Linear(3, 1))\n",
    "# 使用均值0标准差0.01的初始化方法\n",
    "for name, param in net.named_parameters():\n",
    "    if 'weight' in name:\n",
    "        init.normal_(param, mean=0, std=0.01)\n",
    "        print(name, param.data)\n",
    "\n",
    "# 选择全零的初始化方法\n",
    "for name, param in net.named_parameters():\n",
    "    if 'bias' in name:\n",
    "        init.constant_(param, val=0)\n",
    "        print(name, param.data)\n",
    "\n",
    "\n",
    "# 自己定义初始化方法然后进行初始化\n",
    "def init_weight_(tensor):\n",
    "    with torch.no_grad():\n",
    "        tensor.uniform_(-10, 10)\n",
    "        tensor *= (tensor.abs() >= 5).float()\n",
    "\n",
    "\n",
    "for name, param in net.named_parameters():\n",
    "    if 'weight' in name:\n",
    "        init_weight_(param)\n",
    "        print(name, param.data)\n",
    "\n",
    "# 通过改变这些参数的data来改写模型参数值同时不会影响梯度，见之前\n",
    "for name, param in net.named_parameters():\n",
    "    if 'bias' in name:\n",
    "        param.data += 1\n",
    "        print(name, param.data)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "73af2cb1",
   "metadata": {},
   "source": [
    "在有些情况下，我们希望在多个层之间共享模型参数。  \n",
    "4.1.3节提到了如何共享模型参数: Module类的forward函数里多次调用同一个层。  \n",
    "此外，如果我们传入Sequential的模块是同一个Module实例的话参数也是共享的，下面来看一个例子:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "a6733baf",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sequential(\n",
      "  (0): Linear(in_features=1, out_features=1, bias=False)\n",
      "  (1): Linear(in_features=1, out_features=1, bias=False)\n",
      ")\n",
      "0.weight tensor([[3.]])\n",
      "True\n",
      "True\n",
      "tensor(9., grad_fn=<SumBackward0>)\n",
      "tensor([[6.]])\n"
     ]
    }
   ],
   "source": [
    "# 这是一个不带偏倚的线性模型\n",
    "linear = nn.Linear(1, 1, bias=False)\n",
    "net = nn.Sequential(linear, linear)\n",
    "print(net)\n",
    "for name, param in net.named_parameters():\n",
    "    # 初始化成固定值3\n",
    "    init.constant_(param, val=3)\n",
    "    print(name, param.data)\n",
    "\n",
    "print(id(net[0]) == id(net[1]))\n",
    "print(id(net[0].weight) == id(net[1].weight))\n",
    "\n",
    "# 因为模型参数里包含了梯度，所以在反向传播计算时，这些共享的参数的梯度是累加的\n",
    "x = torch.ones(1, 1)\n",
    "y = net(x).sum()\n",
    "print(y)\n",
    "y.backward()\n",
    "print(net[0].weight.grad)  # 单次梯度是3，两次所以就是6"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "381ed623",
   "metadata": {},
   "source": [
    "所以要避免上述情况，则把两个线性层命名为不一样的，否则他们会在内存里指向同一个位置"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "fbc7859e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sequential(\n",
      "  (0): Linear(in_features=1, out_features=1, bias=False)\n",
      "  (1): Linear(in_features=1, out_features=1, bias=False)\n",
      ")\n",
      "0.weight tensor([[3.]])\n",
      "1.weight tensor([[3.]])\n",
      "False\n",
      "False\n",
      "tensor(9., grad_fn=<SumBackward0>)\n",
      "tensor([[3.]])\n"
     ]
    }
   ],
   "source": [
    "# 这是一个不带偏倚的线性模型\n",
    "linear1 = nn.Linear(1, 1, bias=False)\n",
    "linear2 = nn.Linear(1, 1, bias=False)\n",
    "net = nn.Sequential(linear1, linear2)\n",
    "print(net)\n",
    "for name, param in net.named_parameters():\n",
    "    # 初始化成固定值3\n",
    "    init.constant_(param, val=3)\n",
    "    print(name, param.data)\n",
    "\n",
    "# 由于两个模型并不相同，所以二者的id不同\n",
    "print(id(net[0]) == id(net[1]))\n",
    "print(id(net[0].weight) == id(net[1].weight))\n",
    "\n",
    "# 因为模型参数里包含了梯度，所以在反向传播计算时，这些共享的参数的梯度是累加的\n",
    "x = torch.ones(1, 1)\n",
    "y = net(x).sum()\n",
    "print(y)\n",
    "y.backward()\n",
    "print(net[0].weight.grad)  # 这次梯度正常为3"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
