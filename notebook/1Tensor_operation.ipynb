{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "28307b73",
   "metadata": {},
   "source": [
    "## 有关pytorch的Tensor用法"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fe7f5d06",
   "metadata": {},
   "source": [
    "### 1. 导入部分"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "d6337bf9",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import numpy as np\n",
    "\n",
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3ffc75be",
   "metadata": {},
   "source": [
    "### 2. 创建tensor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "806ba7d8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[1.0194e-38, 8.4490e-39, 1.0469e-38],\n",
      "        [9.3674e-39, 9.9184e-39, 8.7245e-39],\n",
      "        [9.2755e-39, 8.9082e-39, 9.9184e-39],\n",
      "        [8.4490e-39, 9.6429e-39, 1.0653e-38],\n",
      "        [1.0469e-38, 4.2246e-39, 1.0378e-38]])\n"
     ]
    }
   ],
   "source": [
    "x = torch.empty(5, 3)\n",
    "print(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "ff90bdc7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[0.2564, 0.0780, 0.7804],\n",
      "        [0.9603, 0.7824, 0.3108],\n",
      "        [0.4476, 0.4636, 0.6215],\n",
      "        [0.8823, 0.4449, 0.8039],\n",
      "        [0.8484, 0.4238, 0.2953]])\n"
     ]
    }
   ],
   "source": [
    "x = torch.rand(5, 3)\n",
    "print(x)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c4a86310",
   "metadata": {},
   "source": [
    "有以下几种\n",
    "1. torch.FloatTensor：32bit float\n",
    "2. torch.DoubleTensor：64bit float\n",
    "3. torch.HalfTensor：16bit float\n",
    "4. torch.ByteTensor：8bit usigned integer\n",
    "5. torch.CharTensor：8bit signed integer\n",
    "6. torch.ShortTensor：16bit int\n",
    "7. torch.IntTensor：32bit int\n",
    "8. torch.LongTensor：64bit int"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "837f8e8b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[0, 0, 0],\n",
      "        [0, 0, 0],\n",
      "        [0, 0, 0],\n",
      "        [0, 0, 0],\n",
      "        [0, 0, 0]])\n"
     ]
    }
   ],
   "source": [
    "x = torch.zeros(5, 3, dtype=torch.long)\n",
    "print(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "2b980026",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([5.5000, 3.0000])\n"
     ]
    }
   ],
   "source": [
    "x = torch.tensor([5.5, 3])\n",
    "print(x)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2713527f",
   "metadata": {},
   "source": [
    "randn_like即用randn生成一个新tensor，其规格跟里面第一个变量一样"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "8c84acb9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[1., 1., 1.],\n",
      "        [1., 1., 1.],\n",
      "        [1., 1., 1.],\n",
      "        [1., 1., 1.],\n",
      "        [1., 1., 1.]], dtype=torch.float64)\n",
      "tensor([[ 0.8663,  0.8838, -1.9079],\n",
      "        [ 1.6581,  0.3732, -1.1469],\n",
      "        [ 1.0164, -0.7236,  0.7262],\n",
      "        [-0.6785, -1.1484, -0.1730],\n",
      "        [ 0.6678, -1.2591,  0.6869]])\n",
      "torch.Size([5, 3])\n",
      "torch.Size([5, 3])\n"
     ]
    }
   ],
   "source": [
    "x = x.new_ones(5, 3, dtype=torch.float64)  # 返回的tensor默认具有相同的torch.dtype和torch.device\n",
    "print(x)\n",
    "\n",
    "x = torch.randn_like(x, dtype=torch.float) # 指定新的数据类型\n",
    "print(x) \n",
    "print(x.size())\n",
    "print(x.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "faa4482b",
   "metadata": {},
   "source": [
    "torch.Tensor(里面放数据即列表（不推荐），或者规格shape)\n",
    "torch.tensor(里面只能放数据即列表)\n",
    "可以由numpy直接导入成tensor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "6eea4cec",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[1. 1. 1.]\n",
      " [1. 1. 1.]]\n",
      "a = tensor([[1., 1., 1.],\n",
      "        [1., 1., 1.]], dtype=torch.float64) \n",
      " a_TYPE = torch.DoubleTensor a_SHAPE =  torch.Size([2, 3])\n",
      "[2 3]\n",
      "a = tensor([2, 3], dtype=torch.int32) \n",
      " a_TYPE = torch.IntTensor a_SHAPE =  torch.Size([2])\n"
     ]
    }
   ],
   "source": [
    "a = np.ones([2, 3])\n",
    "print(a)\n",
    "a = torch.from_numpy(a)\n",
    "print('a =', a, '\\n', 'a_TYPE =', a.type(), 'a_SHAPE = ', a.shape)\n",
    "# 仅仅一个向量\n",
    "a = np.array([2, 3])\n",
    "print(a)\n",
    "a = torch.from_numpy(a)\n",
    "print('a =', a, '\\n', 'a_TYPE =', a.type(), 'a_SHAPE = ', a.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c9c21748",
   "metadata": {},
   "source": [
    "还有很多函数可以创建Tensor，去翻翻官方API就知道了，下表给了一些常用的作参考。\n",
    "\n",
    "函数|功能\n",
    "--|:--:\n",
    "Tensor(*sizes)\t|基础构造函数\n",
    "tensor(data,)\t|类似np.array的构造函数\n",
    "ones(*sizes)\t|全1Tensor\n",
    "zeros(*sizes)\t|全0Tensor\n",
    "eye(*sizes)\t|对角线为1，其他为0\n",
    "arange(s,e,step)\t|从s到e，步长为step\n",
    "linspace(s,e,steps)\t|从s到e，均匀切分成steps份\n",
    "rand/randn(*sizes)\t|均匀/标准分布\n",
    "normal(mean,std)/uniform(from,to)\t|正态分布/均匀分布\n",
    "randperm(m)\t|随机排列"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2978ee8d",
   "metadata": {},
   "source": [
    "### 3. tensor运算"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "4417c1ea",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[ 1.3494,  1.5158, -1.6823],\n",
      "        [ 1.7963,  0.8630, -0.7432],\n",
      "        [ 1.9406,  0.1391,  0.7442],\n",
      "        [-0.5537, -0.9416, -0.1431],\n",
      "        [ 1.0719, -0.9538,  1.0994]])\n"
     ]
    }
   ],
   "source": [
    "y = torch.rand(5, 3)\n",
    "result = torch.empty(5, 3)\n",
    "torch.add(x, y, out=result)\n",
    "print(result)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "58988438",
   "metadata": {},
   "source": [
    "另一种方法，inplace操作（即替换），后面有_做结尾，类似的还有x.copy_(y), x.t_()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "ecfe88fc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[ 1.3494,  1.5158, -1.6823],\n",
      "        [ 1.7963,  0.8630, -0.7432],\n",
      "        [ 1.9406,  0.1391,  0.7442],\n",
      "        [-0.5537, -0.9416, -0.1431],\n",
      "        [ 1.0719, -0.9538,  1.0994]])\n"
     ]
    }
   ],
   "source": [
    "# adds x to y\n",
    "y.add_(x)\n",
    "print(y)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e25eff0b",
   "metadata": {},
   "source": [
    "### 4. tensor索引"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "da8b1c21",
   "metadata": {},
   "source": [
    "下面代码的意义是，把x的第0维度的所有值叫做y，然后给y+1，发现x也随之改变"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "ec9a9002",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[ 0.8663,  0.8838, -1.9079],\n",
      "        [ 1.6581,  0.3732, -1.1469],\n",
      "        [ 1.0164, -0.7236,  0.7262],\n",
      "        [-0.6785, -1.1484, -0.1730],\n",
      "        [ 0.6678, -1.2591,  0.6869]])\n",
      "tensor([ 1.8663,  1.8838, -0.9079])\n",
      "tensor([ 1.8663,  1.8838, -0.9079])\n"
     ]
    }
   ],
   "source": [
    "print(x)\n",
    "y = x[0, :]\n",
    "y += 1\n",
    "print(y)\n",
    "print(x[0, :]) # 源tensor也被改了"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "fc25b61c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[[[ 1.0146, -0.5719,  0.6156, -0.8009],\n",
      "          [ 0.2834, -0.5362,  0.8377, -0.1875],\n",
      "          [ 0.1118, -0.8197, -0.6307, -0.4659]],\n",
      "\n",
      "         [[ 0.0371, -0.9249,  0.1584,  0.1074],\n",
      "          [ 1.6898, -0.5705, -1.2253,  0.4992],\n",
      "          [ 0.5658, -0.7758, -0.6617,  0.2723]]]])\n",
      "tensor([[[ 1.0146, -0.5719,  0.6156, -0.8009],\n",
      "         [ 0.2834, -0.5362,  0.8377, -0.1875],\n",
      "         [ 0.1118, -0.8197, -0.6307, -0.4659]],\n",
      "\n",
      "        [[ 0.0371, -0.9249,  0.1584,  0.1074],\n",
      "         [ 1.6898, -0.5705, -1.2253,  0.4992],\n",
      "         [ 0.5658, -0.7758, -0.6617,  0.2723]]])\n",
      "tensor([[ 0.0371, -0.9249,  0.1584,  0.1074],\n",
      "        [ 1.6898, -0.5705, -1.2253,  0.4992],\n",
      "        [ 0.5658, -0.7758, -0.6617,  0.2723]])\n",
      "tensor([ 1.6898, -0.5705, -1.2253,  0.4992])\n",
      "tensor(-0.5705)\n"
     ]
    }
   ],
   "source": [
    "a = torch.randn(1, 2, 3, 4)\n",
    "print(a)\n",
    "# 直接索引\n",
    "print(a[0])\n",
    "print(a[0, 1])\n",
    "print(a[0, 1, 1])\n",
    "print(a[0, 1, 1, 1])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7dbc1ac2",
   "metadata": {},
   "source": [
    "与python类似，设某数组（list类型）为x\n",
    "1. x[:3]代表取x从0到3不包含3一共3个数，x[1:3]同理\n",
    "2. x[3:]表示3往后的所有元素包括3\n",
    "3. x[-2:]-2为反索引，一个数组，正着数索引是0、1、2、3……反着数是-1（最后一位）、-2（倒数第二位）、-3、-4……（无0），所以本例子的意思是拿出从-2位到最后的数值，即-2和-1\n",
    "4. :是一个三元操作符，x[0:4:2]表示0-4，步长为2，三元操作数的三个数可省略，第一个数默认为0，第二个数默认为end值，第三个数默认为1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "17851d5d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([0.5658, 0.2723])\n"
     ]
    }
   ],
   "source": [
    "print(a[0,1,2,::3])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "18d44a87",
   "metadata": {},
   "source": [
    "除了常用的索引选择数据之外，PyTorch还提供了一些高级的选择函数:\n",
    "\n",
    "函数|功能\n",
    "--|:--:\n",
    "index_select(input, dim, index)\t|在指定维度dim上选取，比如选取某些行、某些列\n",
    "masked_select(input, mask)\t|例子如上，a[a>0]，使用ByteTensor进行选取\n",
    "nonzero(input)\t|非0元素的下标\n",
    "gather(input, dim, index)\t|根据index，在dim维度上选取数据，输出的size与index一样"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ff899ae3",
   "metadata": {},
   "source": [
    "index_select，下列代码的意义为，选择a(1, 2, 3, 4)索引为2的那个维度(即3)，然后在这3里面拿索引为0和2的那俩"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "ab0246b5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[[[ 1.0146, -0.5719,  0.6156, -0.8009],\n",
      "          [ 0.2834, -0.5362,  0.8377, -0.1875],\n",
      "          [ 0.1118, -0.8197, -0.6307, -0.4659]],\n",
      "\n",
      "         [[ 0.0371, -0.9249,  0.1584,  0.1074],\n",
      "          [ 1.6898, -0.5705, -1.2253,  0.4992],\n",
      "          [ 0.5658, -0.7758, -0.6617,  0.2723]]]])\n",
      "tensor([[[[ 1.0146, -0.5719,  0.6156, -0.8009],\n",
      "          [ 0.1118, -0.8197, -0.6307, -0.4659]],\n",
      "\n",
      "         [[ 0.0371, -0.9249,  0.1584,  0.1074],\n",
      "          [ 0.5658, -0.7758, -0.6617,  0.2723]]]])\n",
      "tensor([[[[ 1.0146, -0.5719,  0.6156, -0.8009],\n",
      "          [ 0.2834, -0.5362,  0.8377, -0.1875]],\n",
      "\n",
      "         [[ 0.0371, -0.9249,  0.1584,  0.1074],\n",
      "          [ 1.6898, -0.5705, -1.2253,  0.4992]]]])\n"
     ]
    }
   ],
   "source": [
    "print(a)\n",
    "print(a.index_select(2, torch.tensor([0, 2])))\n",
    "print(a.index_select(2, torch.arange(2)))  # arange(2)即拿出索引0和1"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "43930bcd",
   "metadata": {},
   "source": [
    "...代表省略任意多的维度，视具体情况"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "f8c32e75",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[[[ 1.0146, -0.5719,  0.6156, -0.8009],\n",
      "          [ 0.2834, -0.5362,  0.8377, -0.1875],\n",
      "          [ 0.1118, -0.8197, -0.6307, -0.4659]],\n",
      "\n",
      "         [[ 0.0371, -0.9249,  0.1584,  0.1074],\n",
      "          [ 1.6898, -0.5705, -1.2253,  0.4992],\n",
      "          [ 0.5658, -0.7758, -0.6617,  0.2723]]]])\n",
      "tensor([[ 0.6156,  0.8377, -0.6307],\n",
      "        [ 0.1584, -1.2253, -0.6617]])\n",
      "tensor([[ 0.6156,  0.8377, -0.6307],\n",
      "        [ 0.1584, -1.2253, -0.6617]])\n"
     ]
    }
   ],
   "source": [
    "print(a)\n",
    "print(a[0, :, :, 2])\n",
    "print(a[0, ..., 2])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "35a23ec8",
   "metadata": {},
   "source": [
    "用罩子筛选合格的，然后铺平"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "cce26d64",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[-1.2406,  2.0205, -0.2652,  1.1667],\n",
      "        [-0.4649, -2.3626,  0.9680, -0.1199],\n",
      "        [-1.4365,  0.8207, -2.7220, -0.3868]])\n",
      "tensor([[False,  True, False,  True],\n",
      "        [False, False,  True, False],\n",
      "        [False,  True, False, False]])\n",
      "tensor([2.0205, 1.1667, 0.9680, 0.8207])\n"
     ]
    }
   ],
   "source": [
    "x = torch.randn(3, 4)\n",
    "print(x)\n",
    "mask = x.ge(0) # ge:greater&equal\n",
    "print(mask)\n",
    "result = torch.masked_select(x, mask)\n",
    "print(result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "00cb9c36",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[ 1.4163, -0.1488, -1.3667],\n",
      "        [ 0.3497, -0.3696, -1.4712],\n",
      "        [-1.0615,  1.7108, -0.2360],\n",
      "        [-1.9929, -0.7716,  0.1674],\n",
      "        [-0.3834, -1.0039,  0.7088]])\n",
      "torch.Size([5, 3]) torch.Size([15]) torch.Size([3, 5])\n",
      "tensor([ 1.4163, -0.1488, -1.3667,  0.3497, -0.3696, -1.4712, -1.0615,  1.7108,\n",
      "        -0.2360, -1.9929, -0.7716,  0.1674, -0.3834, -1.0039,  0.7088])\n",
      "tensor([[ 1.4163, -0.1488, -1.3667,  0.3497, -0.3696],\n",
      "        [-1.4712, -1.0615,  1.7108, -0.2360, -1.9929],\n",
      "        [-0.7716,  0.1674, -0.3834, -1.0039,  0.7088]])\n"
     ]
    }
   ],
   "source": [
    "x = torch.randn(5, 3)\n",
    "print(x)\n",
    "y = x.view(15)\n",
    "z = x.view(-1, 5)  # -1所指的维度可以根据其他维度的值推出来\n",
    "print(x.size(), y.size(), z.size())\n",
    "print(y)\n",
    "print(z)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "10a7705a",
   "metadata": {},
   "source": [
    "注意张量之间的关联性，接上一段代码"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "c5a99a29",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[-0.1285,  0.1721, -0.9357],\n",
      "        [ 0.3765, -0.4782, -1.2447],\n",
      "        [-2.9380,  0.4827,  0.4531],\n",
      "        [ 0.4134,  0.6425, -0.7259],\n",
      "        [-0.0984,  0.6953, -0.3944]])\n",
      "tensor([-0.1285,  0.1721, -0.9357,  0.3765, -0.4782, -1.2447, -2.9380,  0.4827,\n",
      "         0.4531,  0.4134,  0.6425, -0.7259, -0.0984,  0.6953, -0.3944])\n",
      "tensor([[ 0.8715,  1.1721,  0.0643],\n",
      "        [ 1.3765,  0.5218, -0.2447],\n",
      "        [-1.9380,  1.4827,  1.4531],\n",
      "        [ 1.4134,  1.6425,  0.2741],\n",
      "        [ 0.9016,  1.6953,  0.6056]])\n",
      "tensor([ 0.8715,  1.1721,  0.0643,  1.3765,  0.5218, -0.2447, -1.9380,  1.4827,\n",
      "         1.4531,  1.4134,  1.6425,  0.2741,  0.9016,  1.6953,  0.6056])\n"
     ]
    }
   ],
   "source": [
    "x = torch.randn(5, 3)\n",
    "y = x.view(15)\n",
    "print(x)\n",
    "print(y)\n",
    "x += 1\n",
    "print(x)\n",
    "print(y) # 也加了1"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f4efc2cf",
   "metadata": {},
   "source": [
    "如果想完全克隆新的要用clone函数"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "9ca13174",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[-0.9657, -0.5606, -0.5309],\n",
      "        [-1.3546, -0.0455,  0.5800],\n",
      "        [ 0.2351,  2.4119,  1.2232],\n",
      "        [-0.9601, -0.6223, -0.7039],\n",
      "        [-0.2179,  1.0267, -0.2830]])\n",
      "tensor([-0.9657, -0.5606, -0.5309, -1.3546, -0.0455,  0.5800,  0.2351,  2.4119,\n",
      "         1.2232, -0.9601, -0.6223, -0.7039, -0.2179,  1.0267, -0.2830])\n",
      "tensor([[-1.9657, -1.5606, -1.5309],\n",
      "        [-2.3546, -1.0455, -0.4200],\n",
      "        [-0.7649,  1.4119,  0.2232],\n",
      "        [-1.9601, -1.6223, -1.7039],\n",
      "        [-1.2179,  0.0267, -1.2830]])\n",
      "tensor([-0.9657, -0.5606, -0.5309, -1.3546, -0.0455,  0.5800,  0.2351,  2.4119,\n",
      "         1.2232, -0.9601, -0.6223, -0.7039, -0.2179,  1.0267, -0.2830])\n"
     ]
    }
   ],
   "source": [
    "x = torch.randn(5, 3)\n",
    "x_cp = x.clone().view(15)\n",
    "print(x)\n",
    "print(x_cp)\n",
    "x -= 1\n",
    "print(x)\n",
    "print(x_cp)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1ac65ee8",
   "metadata": {},
   "source": [
    "另外，PyTorch还支持一些线性函数，这里提一下，免得用起来的时候自己造轮子，具体用法参考官方文档。如下表所示：\n",
    "\n",
    "函数|功能\n",
    "--|--\n",
    "trace\t|对角线元素之和(矩阵的迹)\n",
    "diag\t|对角线元素\n",
    "triu/tril\t|矩阵的上三角/下三角，可指定偏移量\n",
    "mm/bmm\t|矩阵乘法，batch的矩阵乘法\n",
    "addmm/addbmm/addmv/addr/baddbmm..\t|矩阵运算\n",
    "t\t|转置\n",
    "dot/cross\t|内积/外积\n",
    "inverse\t|求逆矩阵\n",
    "svd\t|奇异值分解"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5a74a6bc",
   "metadata": {},
   "source": [
    "### 5. 广播\n",
    "两个形状不同的Tensor按元素运算时，可能会触发广播（broadcasting）机制"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "f11ae5fa",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[1, 2]])\n",
      "tensor([[1],\n",
      "        [2],\n",
      "        [3]])\n",
      "tensor([[2, 3],\n",
      "        [3, 4],\n",
      "        [4, 5]])\n"
     ]
    }
   ],
   "source": [
    "# arange(start,end,step)\n",
    "x = torch.arange(1, 3).view(1, 2)\n",
    "print(x)\n",
    "y = torch.arange(1, 4).view(3, 1)\n",
    "print(y)\n",
    "print(x + y)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "285ba3c4",
   "metadata": {},
   "source": [
    "用y[:] = y + x来覆盖原来的y而不至于去开新的内存空间，有助于节省"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "87844897",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "False\n",
      "True\n"
     ]
    }
   ],
   "source": [
    "x = torch.tensor([1, 2])\n",
    "y = torch.tensor([3, 4])\n",
    "id_before = id(y)\n",
    "y = y + x\n",
    "print(id(y) == id_before)  # False\n",
    "\n",
    "x = torch.tensor([1, 2])\n",
    "y = torch.tensor([3, 4])\n",
    "id_before = id(y)\n",
    "y[:] = y + x\n",
    "# 同效果\n",
    "# y += x\n",
    "# torch.add(x, y, out=y)\n",
    "# y.add_(x)\n",
    "print(id(y) == id_before)  # True"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1e04e91b",
   "metadata": {},
   "source": [
    "### 6. Tensor和NumPy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "d3e9fdae",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([1., 1., 1., 1., 1.]) [1. 1. 1. 1. 1.]\n",
      "tensor([2., 2., 2., 2., 2.]) [2. 2. 2. 2. 2.]\n",
      "tensor([3., 3., 3., 3., 3.]) [3. 3. 3. 3. 3.]\n",
      "[1. 1. 1. 1. 1.] tensor([1., 1., 1., 1., 1.], dtype=torch.float64)\n",
      "[2. 2. 2. 2. 2.] tensor([2., 2., 2., 2., 2.], dtype=torch.float64)\n",
      "[3. 3. 3. 3. 3.] tensor([3., 3., 3., 3., 3.], dtype=torch.float64)\n"
     ]
    }
   ],
   "source": [
    "a = torch.ones(5)\n",
    "b = a.numpy()\n",
    "print(a, b)\n",
    "a += 1\n",
    "print(a, b)\n",
    "b += 1\n",
    "print(a, b)\n",
    "\n",
    "a = np.ones(5)\n",
    "b = torch.from_numpy(a)\n",
    "print(a, b)\n",
    "a += 1\n",
    "print(a, b)\n",
    "b += 1\n",
    "print(a, b)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "93adfa1b",
   "metadata": {},
   "source": [
    "### 7. GPU"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "98f6a99f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([2, 3], device='cuda:0')\n",
      "tensor([2., 3.], dtype=torch.float64)\n"
     ]
    }
   ],
   "source": [
    "# 以下代码只有在PyTorch GPU版本上才会执行\n",
    "if torch.cuda.is_available():\n",
    "    device = torch.device(\"cuda\")  # GPU\n",
    "    y = torch.ones_like(x, device=device)  # 直接创建一个在GPU上的Tensor\n",
    "    x = x.to(device)  # 等价于 .to(\"cuda\")\n",
    "    z = x + y\n",
    "    print(z)\n",
    "    print(z.to(\"cpu\", torch.double))  # to()还可以同时更改数据类型"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3069c2ff",
   "metadata": {},
   "source": [
    "## 自动求梯度"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "909a9a7a",
   "metadata": {},
   "source": [
    "### 1. 基础操作"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "61232a21",
   "metadata": {},
   "source": [
    "加了requires_grad=True那它就被“跟踪”了\n",
    "注意\n",
    "1. 梯度只能对标量求，对张量求就需要输入一个同规格的张量然后将此张量跟要求的张量对位加权乘处理为标量然后求（基本很少这么干）\n",
    "2. 每次反向传播要先调用zero_的inplace方法将梯度清零，不然会累加"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "e1ca5619",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[1., 1.],\n",
      "        [1., 1.]], requires_grad=True)\n",
      "None\n",
      "tensor([[3., 3.],\n",
      "        [3., 3.]], grad_fn=<AddBackward0>)\n",
      "<AddBackward0 object at 0x000002A2A932F7C8>\n",
      "tensor([[27., 27.],\n",
      "        [27., 27.]], grad_fn=<MulBackward0>)\n",
      "tensor(27., grad_fn=<MeanBackward0>)\n",
      "tensor([[1., 1.],\n",
      "        [1., 1.]], requires_grad=True)\n",
      "tensor([[4.5000, 4.5000],\n",
      "        [4.5000, 4.5000]])\n",
      "tensor([[5.5000, 5.5000],\n",
      "        [5.5000, 5.5000]])\n",
      "tensor([[1., 1.],\n",
      "        [1., 1.]])\n"
     ]
    }
   ],
   "source": [
    "x = torch.ones(2, 2, requires_grad=True)\n",
    "print(x)\n",
    "print(x.grad_fn)\n",
    "\n",
    "y = x + 2\n",
    "print(y)\n",
    "print(y.grad_fn)\n",
    "\n",
    "z = y * y * 3\n",
    "# z.mean就是返回矩阵所有元素加和的平均值\n",
    "out = z.mean()\n",
    "print(z)\n",
    "print(out)\n",
    "print(x)\n",
    "out.backward()  # 等价于 out.backward(torch.tensor(1.))\n",
    "print(x.grad)\n",
    "\n",
    "# 再来反向传播一次，注意grad是累加的\n",
    "out2 = x.sum()\n",
    "out2.backward()\n",
    "print(x.grad)\n",
    "\n",
    "out3 = x.sum()\n",
    "x.grad.data.zero_()\n",
    "out3.backward()\n",
    "print(x.grad)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fb6577fc",
   "metadata": {},
   "source": [
    "假设y由自变量x计算而来，w是和y同形的张量，则y.backward(w)的含义是：先计算l = torch.sum(y * w)，则l是个标量，然后求l对自变量x的导数。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "c184c9ac",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[2., 4.],\n",
      "        [6., 8.]], grad_fn=<ViewBackward>)\n",
      "tensor([2.0000, 0.2000, 0.0200, 0.0020])\n"
     ]
    }
   ],
   "source": [
    "x = torch.tensor([1.0, 2.0, 3.0, 4.0], requires_grad=True)\n",
    "y = 2 * x\n",
    "z = y.view(2, 2)\n",
    "print(z)\n",
    "# 现在 z 不是一个标量，所以在调用backward时需要传入一个和z同形的权重向量进行加权求和得到一个标量。\n",
    "v = torch.tensor([[1.0, 0.1], [0.01, 0.001]], dtype=torch.float)\n",
    "z.backward(v)\n",
    "print(x.grad)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "156ebce7",
   "metadata": {},
   "source": [
    "默认是不跟踪的，如果想要开始跟踪这个变量则应该使用inplace方法（带个下划线）来进行属性替换"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "38ad2113",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "False\n",
      "True\n",
      "<SumBackward0 object at 0x000002A2A930BC88>\n"
     ]
    }
   ],
   "source": [
    "a = torch.randn(2, 2)  # 缺失情况下默认 requires_grad = False\n",
    "a = ((a * 3) / (a - 1))\n",
    "print(a.requires_grad)  # False\n",
    "a.requires_grad_(True)\n",
    "print(a.requires_grad)  # True\n",
    "b = (a * a).sum()\n",
    "print(b.grad_fn)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "67e08bcc",
   "metadata": {},
   "source": [
    "### 2. 中断梯度追踪"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "48d571b6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "True\n",
      "tensor(1., grad_fn=<PowBackward0>) True\n",
      "tensor(1.) False\n",
      "tensor(2., grad_fn=<AddBackward0>) True\n",
      "tensor(2.)\n"
     ]
    }
   ],
   "source": [
    "x = torch.tensor(1.0, requires_grad=True)\n",
    "y1 = x ** 2\n",
    "with torch.no_grad():\n",
    "    y2 = x ** 3\n",
    "y3 = y1 + y2\n",
    "\n",
    "# 原本为y3=x^2+x^3\n",
    "# 但是由于y2被阻断了，所以计算梯度时将y2刨除在外（类似于把y2当成了常量）\n",
    "# 结论就是2\n",
    "\n",
    "print(x.requires_grad)\n",
    "print(y1, y1.requires_grad)  # True\n",
    "print(y2, y2.requires_grad)  # False\n",
    "print(y3, y3.requires_grad)  # True\n",
    "\n",
    "y3.backward()\n",
    "print(x.grad)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "028e7e2a",
   "metadata": {},
   "source": [
    "此外，如果我们想要修改tensor的数值，但是又不希望被autograd记录（即不会影响反向传播），那么我们可以对tensor.data进行操作。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "17000960",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([1.])\n",
      "False\n",
      "tensor([100.], requires_grad=True)\n",
      "tensor([2.])\n"
     ]
    }
   ],
   "source": [
    "x = torch.ones(1, requires_grad=True)\n",
    "\n",
    "print(x.data)  # 还是一个tensor\n",
    "print(x.data.requires_grad)  # 但是已经是独立于计算图之外，所以此处应为false\n",
    "\n",
    "y = 2 * x\n",
    "x.data *= 100  # 只改变了值，不会记录在计算图，所以不会影响梯度传播\n",
    "\n",
    "y.backward()\n",
    "print(x)  # 更改data的值也会影响tensor的值\n",
    "print(x.grad)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d91cc3af",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
